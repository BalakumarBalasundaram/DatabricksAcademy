
## Learning objectives
Design databases and pipelines optimized for the Databricks Lakehouse Platform.

Implement efficient incremental data processing to validate and enrich data driving business decisions and applications.

Leverage Databricks-native features for managing access to sensitive data and fulfilling right-to-be-forgotten requests.

Manage code promotion, task orchestration, and production job monitoring using Databricks tools.


## Prerequisites
Experience using PySpark APIs to perform advanced data transformations

Familiarity implementing classes with Python

Experience using SQL in production data warehouse or data lake implementations

Experience working in Databricks notebooks and configuring clusters

Familiarity with creating and manipulating data in Delta Lake tables with SQL

Ability to use Spark Structured Streaming to incrementally read from a Delta table